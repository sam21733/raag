# -*- coding: utf-8 -*-
"""RAG020925.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_FDR_9rgdHBJcitaHI4-BJjJRtJUk9vh
"""

!pip install -q transformers sentence-transformers langchain openai faiss-cpu
!pip install --qu pypdf
!pip install -U langchain-community
!pip install requests

!pip install --qu langchain-google-genai

!pip install --qu  python-dotenv

from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Example: Upload a PDF file to your Colab environment
# Use the file uploader on the left-hand side in Colab
# e.g., 'your_document.pdf'

# 1. Load the document
loader = PyPDFLoader("/content/GENERATIVEAIFORSCIENTIFICDISCOVERY.pdf")
documents = loader.load()

# 2. Split the document into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    add_start_index=True,
)
docs = text_splitter.split_documents(documents)

print(f"Number of document chunks: {len(docs)}")
print(f"Sample chunk: {docs[0].page_content[:200]}...")

from langchain.embeddings import SentenceTransformerEmbeddings

# Choose an embedding model from Hugging Face
# 'all-MiniLM-L6-v2' is a good, lightweight choice
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embedding_function = SentenceTransformerEmbeddings(model_name=embedding_model_name)

from langchain.vectorstores import FAISS

# Create the FAISS vector store from the document chunks and embeddings
vector_store = FAISS.from_documents(docs, embedding_function)

# You can save the vector store for later use
# vector_store.save_local("faiss_index")

# To load it later:
# from langchain.vectorstores import FAISS
# vector_store = FAISS.load_local("faiss_index", embedding_function)

import os
from langchain_google_genai import ChatGoogleGenerativeAI

from dotenv import load_dotenv
import os

load_dotenv()
api_key=os.environ.get("GOOGLE_API_KEY")
print(api_key)

from google.colab import userdata
api_key=userdata.get("GOOGLE_API_KEY")
print(api_key)

import os
from langchain_google_genai import ChatGoogleGenerativeAI

from langchain.chains import RetrievalQA


# --- IMPORTANT: Set your API key ---
# Go to https://platform.openai.com/account/api-keys
# In Colab, you can do this securely:
# from google.colab import userdata
# os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')

# If you don't use the secret manager, you can paste it directly
# os.environ['OPENAI_API_KEY'] = 'your_openai_api_key_here'


# Create the LLM instance
llm = ChatGoogleGenerativeAI(temperature=0, model="gemini-1.5-flash", api_key=api_key)

# Set up the retriever
retriever = vector_store.as_retriever()

# Create the RAG chain
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True,
)

# The user's query
query = "What is the main topic of the document?"

# Run the RAG chain
response = qa_chain({"query": query})

# Print the results
print("Query:", response['query'])
print("Answer:", response['result'])
print("\n--- Source Documents ---")
for doc in response['source_documents']:
    print(f"Content: {doc.page_content[:200]}...")
    print(f"Source: {doc.metadata['source']}")
    print("-" * 20)

